# -*- coding: utf-8 -*-
"""dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15lyXSu06z14MKsLJS0LZmEVRnh7U5Hrz
"""

from pathlib import Path
import os
import pandas as pd
import torch
from torch.utils.data import Dataset
import nltk

import os
import pandas as pd
import torch
from torch.utils.data import Dataset
import nltk

class VQGTensorDataset(Dataset):
    def __init__(self, csv_path, vocab, max_length=20, tensor_dir="/Users/jacobfernandez/Desktop/CornellMEngSP25/DeepLearning/FinalProject/github/data/gqa_resnet_data"):
        # self.df = pd.read_csv(csv_path)
        self.df = csv_path
        self.vocab = vocab
        self.max_length = max_length
        self.tensor_dir = tensor_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        tensor_path = os.path.join(self.tensor_dir, f"{row['tensor_path']}")
        
        if not os.path.exists(tensor_path):
            print(f"‚ùå Missing file: {tensor_path}")
            raise FileNotFoundError(f"Tensor not found at {tensor_path}")
    
        tensor = torch.load(tensor_path)
        tensor = torch.load(tensor_path).float()
        question = str(row["questions"]).strip().lower()
        tokens = nltk.word_tokenize(question)
        indexed_tokens = [self.vocab.get(token, self.vocab["<unk>"]) for token in tokens]
        padded_tokens = indexed_tokens[:self.max_length] + [self.vocab["<pad>"]] * max(0, self.max_length - len(indexed_tokens))

        return tensor, torch.tensor(padded_tokens), question







def load_csv_paths(base_dir):
    """
    Recursively load CSV paths for train/val/test splits under 'bing' and 'flickr'.
    """
    csvs = {"bing": {}, "flickr": {}}

    for root, _, files in os.walk(base_dir):
        for fname in files:
            if not fname.endswith(".csv"):
                continue
            fpath = os.path.join(root, fname)
            if fname.startswith("bing_"):
                if "train" in fname:
                    csvs["bing"]["train"] = fpath
                elif "val" in fname:
                    csvs["bing"]["val"] = fpath
                elif "test" in fname:
                    csvs["bing"]["test"] = fpath
            elif fname.startswith("flickr_"):
                if "train" in fname:
                    csvs["flickr"]["train"] = fpath
                elif "val" in fname:
                    csvs["flickr"]["val"] = fpath
                elif "test" in fname:
                    csvs["flickr"]["test"] = fpath

    return csvs



def create_dataloaders(dataset_paths, vocab, batch_size, max_length):
    loaders = {}
    for domain, splits in dataset_paths.items():
        loaders[domain] = {}
        for split, path in splits.items():
            dataset = VQGTensorDataset(path, vocab, max_length)
            loaders[domain][split] = DataLoader(dataset, batch_size=batch_size, shuffle=(split == "train"))
    return loaders



