# -*- coding: utf-8 -*-
"""dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15lyXSu06z14MKsLJS0LZmEVRnh7U5Hrz
"""

from pathlib import Path
import os
import pandas as pd
import torch
from torch.utils.data import Dataset
import nltk

class VQGTensorDataset(Dataset):
    def __init__(self, csv_path, vocab, max_length=20, tensor_dir="data"):
        self.df = pd.read_csv(csv_path)
        self.vocab = vocab
        self.max_length = max_length
        self.tensor_dir = Path(tensor_dir)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        filename = row["tensor_path"].strip()

        # Route based on filename prefix
        if filename.startswith("flickr") or filename.startswith("bing"):
            base_path = self.tensor_dir /  "final_resnet_data" / Path(filename).name
        elif filename.startswith("coco"):
            base_path = self.tensor_dir / "final_resnet_data" / Path(filename).name
        else:
            base_path = self.tensor_dir / Path(filename).name  # fallback

        tensor_path = base_path.resolve()

        if not tensor_path.exists():
            raise FileNotFoundError(f"Tensor not found at {tensor_path}")

        image_tensor = torch.load(tensor_path).float()

        # Tokenize question
        question = str(row["questions"]).split('---')[0].strip().lower()
        tokens = nltk.word_tokenize(question)
        indices = [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]
        indices = [self.vocab['<start>']] + indices + [self.vocab['<end>']]
        indices = indices[:self.max_length] + [self.vocab['<pad>']] * (self.max_length - len(indices))

        return image_tensor, torch.tensor(indices), question





def load_csv_paths(base_dir):
    """
    Recursively load CSV paths for train/val/test splits under 'bing' and 'flickr'.
    """
    csvs = {"bing": {}, "flickr": {}}

    for root, _, files in os.walk(base_dir):
        for fname in files:
            if not fname.endswith(".csv"):
                continue
            fpath = os.path.join(root, fname)
            if fname.startswith("bing_"):
                if "train" in fname:
                    csvs["bing"]["train"] = fpath
                elif "val" in fname:
                    csvs["bing"]["val"] = fpath
                elif "test" in fname:
                    csvs["bing"]["test"] = fpath
            elif fname.startswith("flickr_"):
                if "train" in fname:
                    csvs["flickr"]["train"] = fpath
                elif "val" in fname:
                    csvs["flickr"]["val"] = fpath
                elif "test" in fname:
                    csvs["flickr"]["test"] = fpath

    return csvs



def create_dataloaders(dataset_paths, vocab, batch_size, max_length):
    loaders = {}
    for domain, splits in dataset_paths.items():
        loaders[domain] = {}
        for split, path in splits.items():
            dataset = VQGTensorDataset(path, vocab, max_length)
            loaders[domain][split] = DataLoader(dataset, batch_size=batch_size, shuffle=(split == "train"))
    return loaders